{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb12eb1a-b5ec-4bc6-8101-b319b8c813bd",
   "metadata": {},
   "source": [
    "# Federated learning on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a4fcb-302a-433b-b618-eb28aeff8e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import radiomics\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from src.helper import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efb23c-fe90-4279-a3f3-9cef5f7dd778",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "We are going to start, as in any other data analytics project, with pre-processing the data to prepare it for modelling. The input data contains CT scans of head & neck patients and clinical data related to them. Radiomic features will be extracted from the CT scans and used as predictors and distance metastasis events will be used as the outcome that we will try to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c5706-57ac-4552-b54c-2200dd362cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "data_path = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f2fd2a-8b8f-40a2-93f5-0f8f84043851",
   "metadata": {},
   "source": [
    "### Clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c8c281-c2f8-45fa-bab4-02f57f8cec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Read the `opc_nodeX_data.csv` file, visualise the first rows, and information about the data\n",
    "# Node 1: `opc_node1_data.csv`\n",
    "# Node 2: `opc_node2_data.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0630e8e-c2c1-4529-9ccb-947ac650c8c4",
   "metadata": {},
   "source": [
    "- What type of data is included?\n",
    "- How many rows and columns?\n",
    "- Can you identify the distance metastasis event column?\n",
    "\n",
    "We are now going to select the patients we are going to use for further analysis. We want to use non-metastatic p16-positive patients and are going to keep the distance metastasis event as outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ee848-006f-427a-a8b6-670ae245b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Filter the data by selecting only the patients we plan to use for further analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1400c9-acaf-4c03-af50-baec381c7194",
   "metadata": {},
   "source": [
    "- How did you filter the data for the patient selection criteria?\n",
    "- How many patients are left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d09584-8fa5-4598-820f-098baac53892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# How many patients are available for the federated analysis?\n",
    "# We are now going to simulate a federated calculation with one iteration\n",
    "# patients_node1 = ?\n",
    "# patients_node2 = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9f1a4-e947-436e-8217-1863130f039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Replace the distance metastasis events with binary values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c91c4-6abb-4448-92ed-cdec52fb2ae1",
   "metadata": {},
   "source": [
    "### Radiotherapy planning CT scans\n",
    "\n",
    "We are now going to extract radiomic features from the CT scans to use as predictors.\n",
    "\n",
    "#### Extracting radiomic features for a single patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e3ede-1a63-467d-b3ae-2fba50530fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CT scans path\n",
    "data_path_scans = os.path.join(data_path, 'CT_scans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095dbc23-aec0-4c2c-a717-aa5f4c00a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CT image\n",
    "id = os.listdir(data_path_scans)[0]\n",
    "slices_path = os.path.join(data_path_scans, id, 'DICOM')\n",
    "slices = load_scan(slices_path)\n",
    "image = get_ct_image(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947fe06-c1ea-469c-a43d-3281e2fe07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get binary mask\n",
    "rt_path = os.path.join(data_path_scans, id, 'RTSTRUCT')\n",
    "rt_file = os.listdir(rt_path)[0]\n",
    "rt_path = os.path.join(rt_path, rt_file)\n",
    "mask = get_gtv_mask(slices, rt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff8738-9b72-4478-b304-342ef72a1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get radiomic features\n",
    "params = os.path.join(os.getcwd(), 'params', 'pyradiomics_params_all.yaml')\n",
    "extractor = radiomics.featureextractor.RadiomicsFeatureExtractor(params)\n",
    "features = pd.Series(extractor.execute(image, mask))\n",
    "print(features.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613683a8-1803-4152-96e1-e5833e7bc133",
   "metadata": {},
   "source": [
    "Explore the information in the `features` series:\n",
    "\n",
    "- Which types of features were extracted?\n",
    "- Besides the features, which other information is available?\n",
    "- Have a look at the `pyradiomics_params_all.yaml` file, which is used to define what will be extracted\n",
    "\n",
    "#### Extracting the radiomic features for all patients\n",
    "\n",
    "As you could see above, `pyradiomics` can extract a multitude of features from the CT scans based on the gross tumour volume (GTV) mask. In order to keep our analysis simple, we are going to select only 5 features: \n",
    "\n",
    "- Surface Area (shape)\n",
    "- Energy (first order)\n",
    "- Cluster Prominence (GLCM)\n",
    "- Large Area Emphasis (GLSZM)\n",
    "- Gray Level Non Uniformity (GLRLM)\n",
    "\n",
    "Go to the `pyradiomics_params.yaml` file in the `params` directory and edit it to extract only the 5 above features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aea92a-adda-447a-8533-7037397194f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "patient_ids = list(df_clinical['Trial PatientID'].values)\n",
    "df_features = pd.DataFrame()\n",
    "for id in patient_ids:\n",
    "    try:\n",
    "        print(f'Extracting radiomics for patient {id}')\n",
    "    \n",
    "        # Get CT image\n",
    "        slices_path = os.path.join(data_path_scans, id, 'DICOM')\n",
    "        slices = load_scan(slices_path)\n",
    "        image = get_ct_image(slices)\n",
    "    \n",
    "        # Get binary mask\n",
    "        rt_path = os.path.join(data_path_scans, id, 'RTSTRUCT')\n",
    "        rt_file = os.listdir(rt_path)[0]\n",
    "        rt_path = os.path.join(rt_path, rt_file)\n",
    "        mask = get_gtv_mask(slices, rt_path)\n",
    "    \n",
    "        # Get radiomic features\n",
    "        params = os.path.join(os.getcwd(), 'params', 'pyradiomics_params.yaml')\n",
    "        extractor = radiomics.featureextractor.RadiomicsFeatureExtractor(params)\n",
    "        features = pd.Series(extractor.execute(image, mask))\n",
    "        \n",
    "        # Organise features in a dataframe\n",
    "        df = pd.DataFrame(features[47:]).T\n",
    "        df['Trial PatientID'] = id\n",
    "        df_features = pd.concat([df_features, df], ignore_index=True)\n",
    "    except:\n",
    "        print(f'No DICOM available for patient {id}')\n",
    "\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1909b0bd-43f1-4b48-809d-7695ce41f4df",
   "metadata": {},
   "source": [
    "## Prepare input data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11500b36-4086-44e6-815e-9c8be8887b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Merge features and outcomes dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80841c-bb05-4f7e-a536-d8ece70f8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Check counts of distant metastasis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a3313-48ad-4abf-9cd1-0415938b3a6e",
   "metadata": {},
   "source": [
    "- What have you noticed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6bc033-bdb1-4ece-87d1-c5e01293e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise features\n",
    "columns = [\n",
    "    'original_shape_SurfaceArea', 'original_firstorder_Energy', 'original_glcm_ClusterProminence', \n",
    "    'original_glszm_LargeAreaEmphasis', 'original_glrlm_GrayLevelNonUniformity'\n",
    "]\n",
    "norms = [32807, 18721084189, 1686168, 270858, 2577]\n",
    "for i in range(len(norms)):\n",
    "    df[columns[i]] = df[columns[i]]/norms[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff90af-477d-4f9f-96e3-27d92db8c816",
   "metadata": {},
   "source": [
    "Notice that you were given normalization constants. In a federated setting the nodes would need to exchange their maximums and minimum and then obtain the overall maximum and minimum. Another option would be to use a common constant, which would not give the guarantee that the variables would be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f3a40d-855c-4b22-ba3b-94dfa27992d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features (X) and outcomes (y)\n",
    "X = df[columns].values\n",
    "y = df['Distant Failure'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716a63c-e45d-4bea-9a7e-d8c9dd8a9e00",
   "metadata": {},
   "source": [
    "## Train federated logistic regression\n",
    "\n",
    "We are now going to simulate a few federated training rounds, so that you can get a better intuition on how it works and how to design the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40412de6-111e-4d7e-a69e-6fb87592f9b2",
   "metadata": {},
   "source": [
    "### Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a349cfa-46bd-48dc-9745-1a5185f97843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data node task\n",
    "# Create local logistic regression model\n",
    "model = LogisticRegression(max_iter=1, warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee54384-3437-4e03-b2d6-488b02d0bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data node task, but initial values are received from central server\n",
    "# Set initial guess\n",
    "model.coef_ = np.array([[0.19947649,  0.33157079, -0.00131289,  0.23688854,  0.26542626]])\n",
    "model.intercept_ = np.array([-0.96108667])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e14936-a7ca-4707-b1a0-c288156ef182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data node task\n",
    "# Fit local model with the training data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c706969-ab14-4a19-bfe0-b22b7317742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check local coefficients\n",
    "print(f'Slopes: {model.coef_}')\n",
    "print(f'Intercept: {model.intercept_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af67dc4-7d98-4dab-b8ab-0ab40d4ca2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central server procedure\n",
    "# Function to aggregate local coefficients into a global model\n",
    "def aggregate_coefficients(wj, nj):\n",
    "    w = np.zeros(len(wj[0]))\n",
    "    N = np.sum(nj)\n",
    "    for p in range(len(wj[0])):\n",
    "        for j in range(len(nj)):\n",
    "            w[p] += nj[j]*wj[j, p]\n",
    "    w = w/N\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a11410-e17a-4356-8daf-02b49bccbc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Central server task\n",
    "# Aggregate coefficients, we are now going to exchange the local coefficients\n",
    "nj = np.array([50, 55])\n",
    "wj = np.array([\n",
    "    [<NODE_1_COEFFICIENTS>],\n",
    "    [<NODE_1_COEFFICIENTS>]\n",
    "])\n",
    "w = aggregate_coefficients(wj, nj)\n",
    "print(f'Round 1')\n",
    "for i in range(len(w)):\n",
    "    print(f'w{i} = {w[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b5b3e-a025-4ba8-b8d4-379d951d3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central server task, result is sent to data nodes\n",
    "# Update global model\n",
    "model.coef_ = np.array([w[1:]])\n",
    "model.intercept_ = np.array([w[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29da62f-4528-4bc2-8e30-44f43f6d8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data node task\n",
    "# Compute local loss\n",
    "loss = log_loss(y, model.predict_proba(X))\n",
    "print(f'Local loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbaea56-c2a8-4e9a-b663-e73063223226",
   "metadata": {},
   "source": [
    "Since we are simulating a few iterations, we are not going to combine the local losses, but we will keep track of how it is evolving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f82e832-491d-4ad6-afd8-4d14e1aa4ccb",
   "metadata": {},
   "source": [
    "### Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0042d-2127-47f7-8e7d-e4bccabb2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Data node task\n",
    "# Fit local model with the training data and new guess for initial coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d09abb-43e2-4b0f-bcfa-f39d4068a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Check local coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9154c-4f87-49ea-a36b-a37a0003bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Central server task\n",
    "# Aggregate coefficients, we are now going to exchange the local coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992f853-d407-4db9-844c-32ca553ffeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Central server task, result is sent to data nodes\n",
    "# Update global model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160062c-a772-47fa-bb91-9fe9649fb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Data node task\n",
    "# Compute local loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea707f-073b-4e93-ac03-f79e77339b97",
   "metadata": {},
   "source": [
    "### Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bb498-2f0d-45d1-9275-d788f3739a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Data node task\n",
    "# Fit local model with the training data and new guess for initial coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e65d5-ebca-4c81-959a-764bd79fd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Check local coefficients, we are now going to exchange the local coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7eeb0-848a-48a1-8005-1e0278c7b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Central server task\n",
    "# Aggregate coefficients, we are now going to exchange the local coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d08ee6-b18b-4ca5-b5fc-a7ee5fe0347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Central server task, result is sent to the researcher as this is the breaking criteria (3 rounds)\n",
    "# Update global model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474207e3-7900-48c8-a7ed-92ecc445c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Data node task\n",
    "# Compute local loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42643304-1e78-46b0-b1c9-704181ee2320",
   "metadata": {},
   "source": [
    "- How did the local loss evolve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f09fd-6b57-43a5-8879-157d604642f7",
   "metadata": {},
   "source": [
    "## Evaluating global model\n",
    "\n",
    "Some data was kept in a simulated validation node. This is the method we are using to evaluate the final federated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba28020-83de-4d97-ba80-03d447c8d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "file_path = os.path.join(data_path, 'opc_test_data.csv')\n",
    "df_test = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa960b1-ec91-4417-a725-0d660a4c36eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Get features and outcomes (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df217f1a-e31a-4053-8eb9-71ffa30ca5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data node task, global model received from the central server\n",
    "# Evaluate model by computing the accuracy\n",
    "print(f'Accuracy: {model.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed5fae-63b3-41b9-8b5f-f942fec16a4e",
   "metadata": {},
   "source": [
    "- What do you think of this result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d6a84-9175-4803-a76b-e233e273ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, model.predict(X_test), labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1b1f8-4459-4d70-8e2e-cd256afb7b68",
   "metadata": {},
   "source": [
    "## Centralised model\n",
    "\n",
    "Finally, let's compare the federated solution to the centralised one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04985236-d42a-4b38-add7-873b68e17295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "file_path = os.path.join(data_path, 'opc_central_reduced_data.csv')\n",
    "df_all = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c9e02-a767-483a-9c42-a8ca86ee0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Get features and outcomes (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73576884-4873-4051-b24a-be53a0976105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Split data into train (80%) and test (20%) sets and use random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=<ADD_SIZE>, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362dce3-904d-41a1-841f-76b93eb0f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Train centralised model using the default hyper-parameters  and visualise coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133833f0-126e-4f03-9ad5-ffad71998c4f",
   "metadata": {},
   "source": [
    "- How do the coefficients compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1d9fe-36c1-4ae8-961c-2ee87c0605b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Evaluate central model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37178f-bcd9-45fd-94e7-f68f9d0b55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10522000-2939-4109-bd7c-e534893c817f",
   "metadata": {},
   "source": [
    "- How do the performances from federated vs. centralised solutions compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935aede0-605a-4ff5-b266-8079666e66b2",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Pydicom: General examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html)\n",
    "- [Pyradiomics documentation](https://pyradiomics.readthedocs.io/en/latest/index.html)\n",
    "- [Ontology-guided Radiomics Analysis Workflow (O-RAW)](https://github.com/zhenweishi/O-RAW)\n",
    "- [Kwan et al. (2018)](https://doi.org/10.1016/j.ijrobp.2018.01.057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2d410-d1d6-4299-8f02-37e8c76c4dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mie2003",
   "language": "python",
   "name": "mie2003"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
